# Hugging face ğŸ¤— llm course ğŸ“š / curso de llm ğŸ“š

https://pytorch.org/get-started/locally/#supported-linux-distributions 

Nesse link deve-se decidir se irÃ¡ usar a CPU ou GPU. Selecionar o SO utilizado e instalar as depedencias essenciais para rodar os elementos a seguir.

## PreparaÃ§Ã£o do ambiente para windows ğŸ’»:

>Terminal: 



    pip install torch 

â„¹ï¸Por padrÃ£o instala uma versÃ£o otimizada para cpu da biblioteca

    python -m venv venv_(nome_da_sua_pasta)
â„¹ï¸ (Verificar)

    venv\Scripts\activate

â„¹ï¸ AtivaÃ§Ã£o do ambiente virtual

    deactivate 

â„¹ï¸ Desativar o ambiente virtual sempre que terminar os estudos 


## PreparaÃ§Ã£o do ambiente para linux ğŸ’»:

>Terminal:  

    sudo apt install python3 python3-pip python3-venv ; 

â„¹ï¸InstalaÃ§Ã£o do python e do ambiente virtual

    python3 -m venv venv_(nome_da_pasta) 

â„¹ï¸ 

    source venv_(nome_da_pasta)/bin/activate  

â„¹ï¸ativar o ambiente isolado para trabalho; O nome da pasta ficarÃ¡ entre parÃªnteses. 





    deactivate
â„¹ï¸Para desativar o ambiente virtual


â„¹ï¸Nota: caso tenha dificuldade para achar o activate (linux) usar:

     "find . -name "activate" 

## PreparaÃ§Ã£o do ambiente para utilizaÃ§Ã£o dos Transformers ğŸ§  :

    pip install transformers / datasets / evaluate / sentencepiec

â„¹ï¸O cÃ³digo acima pode ser instalado por partes por questÃµes de organizaÃ§Ã£o e estudo ex: "pip install transformers" apenas.

â„¹ï¸**Nota: o modelo padrÃ£o Ã© - distilbert/distilbert-base-uncased-finetuned-sst-2-englishand revision 714eb0f** 

â„¹ï¸**Nota: sempre receber o resultado (result=) do objeto para depois imprimir (print(result)).**

## 1Âº Transformer sentiment analysis â•ou â–

--------------------------------

**Teste 1**

    from transformers import pipeline

    classifier = pipeline("sentiment-analysis")
    result = classifier("I've been waiting for a HuggingFace course my whole life.")

    print(result)

SaÃ­da relevanteğŸ“: [{'label': 'POSITIVE', 'score': 0.9598046541213989}]

â„¹ï¸Nota: aprovado.

--------------------------------

**Teste 2**

    from transformers import pipeline

    classifier = pipeline("sentiment-analysis")
    result = classifier("I love instrumental music")

    print(result)

SaÃ­da relevanteğŸ“: [{'label': 'POSITIVE', 'score': 0.9998270869255066}]

â„¹ï¸Nota: aprovado.

--------------------------------

**Teste 3**

    from transformers import pipeline

    classifier = pipeline("sentiment-analysis")
    result = classifier("I admire those who still have the habit of reading")

    print(result)

SaÃ­da relevanteğŸ“:[{'label': 'POSITIVE', 'score': 0.9996665716171265}]

â„¹ï¸Nota: aprovado.

--------------------------------
**Teste 4**

    from transformers import pipeline

    classifier = pipeline("sentiment-analysis")
    result = classifier("I hate apple pie")

    print(result)

SaÃ­da relevanteğŸ“: [{'label': 'NEGATIVE', 'score': 0.9986454844474792}]

â„¹ï¸Nota: aprovado.

--------------------------------

**Teste 5** (Teste em portuguÃªS)

    from transformers import pipeline

    classifier = pipeline("sentiment-analysis")
    result = classifier("eu tenho Ã³dio de uva passas no natal")

    print(result)

SaÃ­da relevanteğŸ“: [{'label': 'NEGATIVE', 'score': 0.9876565933227539}]

â„¹ï¸Nota: aprovado.


--------------------------------
**Teste 6**

    from transformers import pipeline

    classifier = pipeline("sentiment-analysis")
    result = classifier("I have a complaint to make")

    print(result)

SaÃ­da relevanteğŸ“: [{'label': 'POSITIVE', 'score': 0.9828368425369263}]

â„¹ï¸Nota: reprovado

--------------------------------
**Teste 7**

    from transformers import pipeline

    classifier = pipeline("sentiment-analysis")
    result = classifier(["I have a complaint to make",
                        "Eu gostei muito do cholate",
                        "I'm honestly disappointed",
                        "Gostaria de conversar a sÃ³s com vocÃª",
                        "I'll meet you at HR"])

    print(result)

SaÃ­da relevanteğŸ“:

[{'label': 'NEGATIVE', 'score': 0.9967466592788696}, 

â„¹ï¸Nota:aprovado.

{'label': 'NEGATIVE', 'score': 0.9786876440048218}, 

â„¹ï¸Nota:reprovado.

{'label': 'NEGATIVE', 'score': 0.9996434450149536}, 

â„¹ï¸Nota:aprovado.
 

{'label': 'NEGATIVE', 'score': 0.7726762294769287}, 

â„¹ï¸Nota:aprovado.
  
{'label': 'POSITIVE', 'score': 0.9994791150093079}] 

â„¹ï¸Nota:reprovado.

--------------------------------
**Teste 8** 

    from transformers import pipeline

    classifier = pipeline("sentiment-analysis")
    result = classifier(["EU amo vocÃª",
                        "Eu acho meu cunhado estranho",])

    print(result)

SaÃ­da relevanteğŸ“: [{'label': 'POSITIVE', 'score': 0.9777462482452393}, 

â„¹ï¸Nota:aprovado.


SaÃ­da relevanteğŸ“: {'label': 'POSITIVE', 'score': 0.9578021168708801}] 

â„¹ï¸Nota:reprovado.

--------------------------------
**Teste 9** 

    from transformers import pipeline

    classifier = pipeline("sentiment-analysis")
    result = classifier(["MÃ£e, pai eu amo vocÃªs",
                        "Eu odeio aquele cara que estudou comigo no fundamental",])

    print(result)

SaÃ­da relevanteğŸ“: [{'label': 'NEGATIVE', 'score': 0.8256018161773682}, 

â„¹ï¸Nota:reprovado.

SaÃ­da relevanteğŸ“: {'label': 'NEGATIVE', 'score': 0.9777287840843201}]

â„¹ï¸Nota:reprovado.

--------------------------------
**Teste 10** 

from transformers import pipeline

    classifier = pipeline("sentiment-analysis")
    result = classifier(["mom, dad I love you",
                        "I hate that guy I went to elementary school with.",])

    print(result)

SaÃ­da relevanteğŸ“:[{'label': 'POSITIVE', 'score': 0.9998345375061035}, 

â„¹ï¸Nota:aprovado

SaÃ­da relevanteğŸ“:{'label': 'NEGATIVE', 'score': 0.9987708926200867}] 

â„¹ï¸Nota:aprovado.


--------------------------------
**Teste extra**

    from transformers import pipeline

    classifier = pipeline("sentiment-analysis")
    result = classifier(["Before I felt bad about eating 3 slices of pizza every weekend,"
                        " but I think that if I hardly eat sugar during the week and I rarely drink soda then there is "
                        "no problem in eating 3 slices of pizza on the weekend, especially if it is homemade and with my wife."])

    print(result)

SaÃ­da relevanteğŸ“: [{'label': 'POSITIVE', 'score': 0.9828368425369263}] 

â„¹ï¸Nota:aprovado.

--------------------------------


## 2Âº Transformer zero shot classification ğŸ“ŠğŸ”«

âœ¨âœ¨ExercÃ­cio hugging face: âœï¸ Try it out! Play around with your own sequences and labels and see how the model behaves.
âœ¨âœ¨Experimente! Experimente com suas prÃ³prias sequÃªncias e rÃ³tulos e veja como o modelo se comporta.

âœ¨âœ¨Resposta do exercÃ­cio nos testes:

Base:

    from transformers import pipeline

    classifier = pipeline("zero-shot-classification")
    result = classifier(
        "This is a course about the Transformers library",
        candidate_labels=["education", "politics", "business"],
    )

    print(result)

--------------------------------
**Teste 1**

from transformers import pipeline

    classifier = pipeline("zero-shot-classification")
    result = classifier(
        "I would like to know if there is kanikama",
        candidate_labels=["Order", "Menu", "Information"],
    )

    print(result)

SaÃ­da relevanteğŸ“: {'sequence': 'I would like to know if there is kanikama', 'labels': ['Information', 'Order', 'Menu'], 'scores': [0.6081867814064026, 0.21226967871189117, 0.17954352498054504]}

â„¹ï¸Nota: nesse caso era para retornar Menu com maior score. Logo seria reprovado o teste

--------------------------------
***Teste 2** 

    from transformers import pipeline

    classifier = pipeline("zero-shot-classification")
    result = classifier(
        "I would like to know if there is kanikama",
        candidate_labels=["Order", "product availability"],
    )

    print(result)

SaÃ­da relevanteğŸ“: {'sequence': 'I would like to know if there is kanikama', 'labels': ['product availability', 'Order'], 'scores': [0.5289784669876099, 0.471021443605423]}

â„¹ï¸Nota:aprovado.

--------------------------------
**Teste 3** 

    from transformers import pipeline

    classifier = pipeline("zero-shot-classification")
    result = classifier(
        "I would like to know if you deliver to the Tucuna neighborhood",
        candidate_labels=["Order", "product availability","delivery information"],
    )

    print(result)

SaÃ­da relevanteğŸ“: {'sequence': 'I would like to know if you deliver to the Tucuna neighborhood', 'labels': ['delivery information', 'product availability', 'Order'], 'scores': [0.7124969363212585, 0.19233402609825134, 0.09516900032758713]}

â„¹ï¸Nota: aprovado, porÃ©m deve se adicionar rÃ³tulos bem especÃ­ficos (escolher bem as palavras), foram feitos teste para chegar nesse resultado.

--------------------------------
**Teste 4** 

    from transformers import pipeline

    classifier = pipeline("zero-shot-classification")
    result = classifier(
        "I would like to order salmon, kani and tea",
        candidate_labels=["Order", "product availability","delivery information"],
    )

    print(result)

SaÃ­da relevanteğŸ“: {'sequence': 'I would like to order salmon, kani and tea', 'labels': ['Order', 'product availability', 'delivery information'], 'scores': [0.5683993697166443, 0.2863132059574127, 0.145287424325943]}

â„¹ï¸Nota:aprovado, porÃ©m caso colocado o texte em portuguÃªs trouxe o rÃ³tulo delivey information com o maior score.

--------------------------------

**Teste 5** 

    from transformers import pipeline

    classifier = pipeline("zero-shot-classification")
    result = classifier(
        "I would like to know if there are other branches of the store ?",
        candidate_labels=["Order", "product availability","delivery information","store information"],
    )

    print(result)

SaÃ­da relevanteğŸ“: 'labels': ['store information', 'product availability', 'Order', 'delivery information'], 'scores': [0.7502809762954712, 0.11712954193353653, 0.08225210756063461, 0.05033739656209946]

â„¹ï¸Nota:aprovado com um rÃ³tulo a mais.

--------------------------------

## 3Âº transformer text generation. ğŸ’¬ğŸ’­âœ¨

Base:

    from transformers import pipeline

    generator = pipeline("text-generation", model="HuggingFaceTB/SmolLM2-360M")
    generator(
        "In this course, we will teach you how to",
        max_length=30,
        num_return_sequences=2,
    )

â„¹ï¸Nota: A base gera um texto bem mal formatado se for executado de forma crua no editor.

âœ¨âœ¨âœ¨ExercÃ­cio hugging face: âœï¸ Try it out! Use the num_return_sequences and max_length arguments to generate two sentences of 15 words each.

âœ¨âœ¨Experimente! Use os num_return_sequencesargumentos max_lengthe para gerar duas frases de 15 palavras cada.

âœ¨Resposta:

    from transformers import pipeline

    generator = pipeline("text-generation") # Isso usarÃ¡ o modelo padrÃ£o que vocÃª viu

    result = generator(
        "The clouds are sereny and bright,",
        max_new_tokens=15, # Gerar no MÃXIMO 15 NOVOS tokens (aproximadamente 15 palavras)
        num_return_sequences=2, # Comece com 1 para facilitar a depuraÃ§Ã£o da saÃ­da
        do_sample=True, # Ajuda na criatividade e reduz repetiÃ§Ã£o
        temperature=0.7, # Controla a aleatoriedade (entre 0.5 e 1.0 Ã© um bom range)
        pad_token_id=generator.tokenizer.eos_token_id # Importante para que o modelo saiba parar
    )

    print(result)

SaÃ­da relevanteğŸ“: [{'generated_text': 'The clouds are sereny and bright, and the air is so fresh that people can breathe freely without looking down.'}, 


SaÃ­da relevanteğŸ“: {'generated_text': 'The clouds are sereny and bright, and I can see the stars and the stars and the stars. And I'}]

â„¹ï¸Nota: o segundo texto Ã© muito aleatÃ³rio e pode vir atÃ© mesmo incoerente e incompleto diferente do primeiro que tende a ser mais coerente e completo, o porÃ©m nesse teste a saÃ­da Ã© mais organizada.

âœ¨âœ¨âœ¨ExercÃ­cico hugging face: âœï¸ Try it out! Use the filters to find a text generation model for another language. Feel free to play with the widget and use it in a pipeline!/
âœ¨âœ¨Experimente! Use os filtros para encontrar um modelo de geraÃ§Ã£o de texto para outro idioma. Sinta-se Ã  vontade para experimentar o widget e usÃ¡-lo em um pipeline!

âœ¨Resposta:

    from transformers import pipeline

    generator = pipeline("text-generation", model="goldfish-models/por_latn_1000mb")
    result=generator(
        "Nuvens sÃ£o serenas e",
        max_length=30,
        num_return_sequences=1,
    )

    print(result)

SaÃ­da relevanteğŸ“: [{'generated_text': 'Nuvens sÃ£o serenas e Para se ter uma ideia da sua complexidade, o nÃºmero de participantes que irÃ¡ decorrer nas diferentes etapas do concurso de poesia vai ser cada vez menor.'}]

â„¹ï¸Nota: portuguÃªs bom, porÃ©m com um pouco menos de coerÃªncia.



## 4Âº Transformer fill mask ğŸ‘¹ ğŸ‘º ğŸ­

Base: 

    from transformers import pipeline

    unmasker = pipeline("fill-mask")
    unmasker("This course will teach you all about <mask> models.", top_k=2)

âœ¨âœ¨âœ¨ExercÃ­cio hugging face: âœï¸ Try it out! Search for the bert-base-cased model on the Hub and identify its mask word in the Inference API widget. What does this model predict for the sentence in our pipeline example above?

âœ¨âœ¨Experimente! Procure o modelo bert-base-cased no Hub e identifique sua palavra-mÃ¡scara no widget da API de InferÃªncia. O que esse modelo prevÃª para a frase em nosso exemplo de pipeline acima?

âœ¨R:

    from transformers import pipeline

    unmasker = pipeline("fill-mask", model="neuralmind/bert-base-portuguese-cased")
    result=unmasker("This course will teach you all about [MASK] models", top_k=2)

    print(result)

SaÃ­da relevanteğŸ“ [{'score': 0.6997271180152893, 'token': 1621, 'token_str': 'the', 'sequence': 'This course will teach you all about the models'}, 


SaÃ­da relevanteğŸ“: {'score': 0.04927678406238556, 'token': 123, 'token_str': 'a', 'sequence': 'This course will teach you all about a models'}]

## 5Âº Transformer Named entity recognition ğŸ­ ğŸ’ğŸ—¼

Base:

    from transformers import pipeline

    ner = pipeline("ner", grouped_entities=True)
    ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")

âœ¨âœ¨âœ¨ExecÃ­cico hugging face: âœï¸ Try it out! Search the Model Hub for a model able to do part-of-speech tagging (usually abbreviated as POS) in English. What does this model predict for the sentence in the example above? 

âœ¨âœ¨ âœï¸ Experimente! Procure no Model Hub por um modelo capaz de fazer marcaÃ§Ã£o de classes gramaticais (geralmente abreviado como POS) em inglÃªs. O que esse modelo prevÃª para a frase do exemplo acima?

âœ¨R:

    from transformers import pipeline

    ner = pipeline("ner", grouped_entities=True,model="dslim/bert-base-NER")
    result=ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")

    print(result)

SaÃ­da relevanteğŸ“
[
  {'entity_group': 'PER', 'score': 0.9981525, 'word': 'Sylvain', 'start': 11, 'end': 18},
  {'entity_group': 'ORG', 'score': 0.93690395, 'word': 'Hugging Face', 'start': 33, 'end': 45},
  {'entity_group': 'LOC', 'score': 0.9971419, 'word': 'Brooklyn', 'start': 49, 'end': 57}
]

â„¹ï¸Nota: esse conseguiu classificar como pessoa, organizaÃ§Ã£o e local.


**Teste 1:** 

    from transformers import pipeline

    ner = pipeline("ner", grouped_entities=True,model="vblagoje/bert-english-uncased-finetuned-pos")
    result=ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")

    print(result)

SaÃ­da relevanteğŸ“:
[
    {'entity_group': 'PRON', 'score': np.float32(0.9994592), 'word': 'my', 'start': 0, 'end': 2},
    {'entity_group': 'NOUN', 'score': np.float32(0.99601364), 'word': 'name', 'start': 3, 'end': 7},
    {'entity_group': 'AUX', 'score': np.float32(0.9953696), 'word': 'is', 'start': 8, 'end': 10},
    {'entity_group': 'PROPN', 'score': np.float32(0.9981525), 'word': 'sylvain', 'start': 11, 'end': 18},
    {'entity_group': 'CCONJ', 'score': np.float32(0.99918765), 'word': 'and', 'start': 19, 'end': 22},
    {'entity_group': 'PRON', 'score': np.float32(0.9994679), 'word': 'i', 'start': 23, 'end': 24},
    {'entity_group': 'VERB', 'score': np.float32(0.99923587), 'word': 'work', 'start': 25, 'end': 29},
    {'entity_group': 'ADP', 'score': np.float32(0.90630955), 'word': 'at', 'start': 30, 'end': 32},
    {'entity_group': 'PROPN', 'score': np.float32(0.719051), 'word': 'hugging face', 'start': 33, 'end': 45},
    {'entity_group': 'ADP', 'score': np.float32(0.9993789), 'word': 'in', 'start': 46, 'end': 48},
    {'entity_group': 'PROPN', 'score': np.float32(0.9989513), 'word': 'brooklyn', 'start': 49, 'end': 57},
    {'entity_group': 'PUNCT', 'score': np.float32(0.99963903), 'word': '.', 'start': 57, 'end': 58}
]

â„¹ï¸Nota: esse modelo, classificou a sintaxe do texto. (Resposta do exrcÃ­cio)

## 6Âº Transformer Question answering ğŸ¤ğŸ’¬ğŸ’­

    Base:

    from transformers import pipeline

    question_answerer = pipeline("question-answering")
    question_answerer(
        question="Where do I work?",
        result=context="My name is Sylvain and I work at Hugging Face in Brooklyn",
    )

    print(result)

**Teste 1:**

    from transformers import pipeline

    question_answerer = pipeline("question-answering")
    result = question_answerer(
        question="You do delivery ?",
        context="menu,itens: fresh fish,frozen fish, kani and seaweed.," \
        "Order: If you want to place an order, please leave your name, your order,Localization: " \
        "your address and payment method "
        "and we will soon check the availability of the products and more information about the order."\
        "Addresses:Here are the of the stores and their opening hours..."\
        "Delivery informations: We deliver to neighborhoods x, y, x from time x to y, from x to y ..."

    )

    print(result)

â„¹ï¸Nota: Deve-se fazer a pergunta de forma mais direta possÃ­vel para obter o melhor retorno.

## 7Âº Transformer summarization. ğŸ“’ğŸ“ğŸ“–

Base:

    from transformers import pipeline

    summarizer = pipeline("summarization")
    result=summarizer(
        """
        America has changed dramatically during recent years. Not only has the number of 
        graduates in traditional engineering disciplines such as mechanical, civil, 
        electrical, chemical, and aeronautical engineering declined, but in most of 
        the premier American universities engineering curricula now concentrate on 
        and encourage largely the study of engineering science. As a result, there 
        are declining offerings in engineering subjects dealing with infrastructure, 
        the environment, and related issues, and greater concentration on high 
        technology subjects, largely supporting increasingly complex scientific 
        developments. While the latter is important, it should not be at the expense 
        of more traditional engineering.

        Rapidly developing economies such as China and India, as well as other 
        industrial countries in Europe and Asia, continue to encourage and advance 
        the teaching of engineering. Both China and India, respectively, graduate 
        six and eight times as many traditional engineers as does the United States. 
        Other industrial countries at minimum maintain their output, while America 
        suffers an increasingly serious decline in the number of engineering graduates 
        and a lack of well-educated engineers.
    """
    )

    print(result)

{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}

## 8Âº Transformer Translation. Tradutor. ğŸ‡ºğŸ‡¸ > ğŸ‡«ğŸ‡· > ğŸ‡°ğŸ‡·

â„¹ï¸Nota: para esse transformer Ã© usado o sentence piece, caso seja solicitado usar pip install sentecepiece

    from transformers import pipeline

    translator = pipeline("translation", model="Helsinki-NLP/opus-mt-fr-en")
    result=translator("Ce cours est produit par Hugging Face.")

    print(result)

SaÃ­da relevanteğŸ“: [{'translation_text': 'This course is produced by Hugging Face.'}]

âœ¨âœ¨âœ¨ExercÃ­cio hugging face: âœï¸ Try it out! Search for translation models in other languages and try to translate the previous sentence into a few different languages.


âœ¨âœ¨ âœï¸ Experimente! Pesquise modelos de traduÃ§Ã£o em outros idiomas e tente traduzir a frase anterior para vÃ¡rios idiomas diferentes.


âœ¨R:

**(De pt para en)**

    from transformers import pipeline

    translator = pipeline("translation", model="Helsinki-NLP/opus-mt-tc-big-en-pt")
    result=translator("Hi, I would like to make a request")

    print(result)

SaÃ­da relevanteğŸ“: [{'translation_text': 'OlÃ¡, gostaria de fazer um pedido'}]

## 9Âº Transformer image classification ğŸ¦ğŸ“·ğŸ’­

**Nota: esse transformer necessita de um biblioteca chamada pillow, para instalar**
     
     pip install pillow


Base:

    from transformers import pipeline

    image_classifier = pipeline(
        task="image-classification", model="google/vit-base-patch16-224"
    )
    result = image_classifier(
        "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
    )
    print(result)

SaÃ­da relevanteğŸ“:

Device set to use cpu
[{'label': 'lynx, catamount', 'score': 0.43349984288215637}, 
{'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor', 'score': 0.03479618579149246},
{'label': 'snow leopard, ounce, Panthera uncia', 'score': 0.03240193799138069},
{'label': 'Egyptian cat', 'score': 0.02394479140639305},
{'label': 'tiger cat', 'score': 0.022889239713549614}]

**Teste 1:**

    from transformers import pipeline

    image_classifier = pipeline(
        task="image-classification", model="google/vit-base-patch16-224"
    )
    result = image_classifier("imagem local com uma piscina e um mergulhador")
    print(result)

SaÃ­da relevanteğŸ“:

[{'label': 'bathing cap, swimming cap', 'score': 0.5953492522239685}, 
{'label': 'swimming trunks, bathing trunks', 'score': 0.1526799201965332}, 
{'label': 'snorkel', 'score': 0.09047021716833115}, 
{'label': 'maillot, tank suit', 'score': 0.04795584827661514}, 
{'label': 'maillot', 'score': 0.02403387613594532}]

**10Âº Transformer Automatic speech recognition (AtenÃ§Ã£o esse nÃ£o foi possivel rodar) ğŸ”ˆğŸ“£ğŸ“‘**

Nota Ã© necessÃ¡rio: ffmpeg e pip install soundfile librosa

Base:

    from transformers import pipeline

    transcriber = pipeline(
        task="automatic-speech-recognition", model="openai/whisper-large-v3"
    )
    result = transcriber(
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac"
    )
    print(result)

â„¹ï¸Nota: o modelo base utilizado Ã© muito grande logo pode ser substituido por um menor como : openai/whisper-small

**Teste 1:**

    from transformers import pipeline

    transcriber = pipeline( task="automatic-speech-recognition", model="openai/whisper-small" ) 
    result = transcriber( "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac" ) 

    print(result)

SaÃ­da relevanteğŸ“: {'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}

â„¹ï¸Nota: teste aprovado.

## â˜ï¸ â˜ï¸  Pegadas de carbono: 

ğŸ”§ğŸ”§ Ferramentas: https://mlco2.github.io/impact/#compute ; https://codecarbon.io/ ; 

base:

    from codecarbon import EmissionsTracker

    tracker = EmissionsTracker()
    tracker.start()
    GPU Intensive code goes here
    tracker.stop()

Tokenizadores:

base:

    from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")
result = tokenizer.tokenize("I want do a order")

print(result)

SaÃ­da relevanteğŸ“: 

['i', 'want', 'do', 'a', 'order']

## Usando transformadores. ğŸ“„ğŸ“š

## Modulo 2 por trÃ¡s da funÃ§Ã£o pipeline.

## Funcionamento do analisador de sentimentos. ğŸ˜Š ou ğŸ˜¡

â„¹ï¸ Nota todo os cÃ³digos abaixo podem ser copiado juntos para o editor.

## Fase 0: O resultado

    from transformers import pipeline

    classifier = pipeline("sentiment-analysis")
    result = classifier(
        [
            "I've been waiting for a HuggingFace course my whole life.",
            "I hate this so much!",
        ]
    )

    print(result)

SaÃ­da relevante da fase 0 ğŸ“:

    [{'label': 'POSITIVE', 'score': 0.9598046541213989}, {'label': 'NEGATIVE', 'score': 0.9994558691978455}]


## Fase 1: PrÃ©-processamento com um tokenizador

    from transformers import AutoTokenizer


    checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)


    raw_inputs = [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
    inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
    print(inputs)

SaÃ­da relevante da fase 1 ğŸ“:

    {'input_ids': tensor
    
    ([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],

    [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,
    0,     0,     0,     0,     0,  0]]),

     'attention_mask': tensor
     
     ([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
     
     [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}

â„¹ï¸ Nota: A saÃ­da em si Ã© um dicionÃ¡rio contendo duas chaves, input_ids e attention_mask. input_ids contÃ©m duas linhas de inteiros (uma para cada frase) que sÃ£o os identificadores exclusivos dos tokens em cada frase

â„¹ï¸ Nota: os ## no cÃ³digo existem pois todo os cÃ³digos dessa parte foram unificados, logo eles podem ser utilizado juntos no editor, porÃ©m se esse em especÃ­fico for utilizado sÃ³ deve ser removido os ##.

## Fase 2: Passando pelo modelo.

    from transformers import AutoModel

    ##checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
    ##checkpoint ja foi carregado mais acima caso o cÃ³digo seja usado sÃ³, deve-se usar o chackpoint acima

    model = AutoModel.from_pretrained(checkpoint)

    outputs = model(**inputs)
    print(outputs.last_hidden_state.shape)

SaÃ­da relevante da fase 2 ğŸ“:


    torch.Size([2, 16, 768])




â„¹ï¸Nota: A saÃ­da sera trÃªs nÃºmeros que tem o significado abaixo:

- Tamanho do lote: O nÃºmero de sequÃªncias processadas por vez (2 em nosso exemplo).

- Comprimento da sequÃªncia: O comprimento da representaÃ§Ã£o numÃ©rica da sequÃªncia (16 no nosso exemplo).

- Tamanho oculto: A dimensÃ£o vetorial de cada entrada do modelo (768 em nosso exemplo).

## Fase 2.1 CabeÃ§ote do modelo: Dando sentido aos nÃºmeros

    from transformers import AutoModelForSequenceClassification

    ## Modelo com um cabeÃ§alho de classificaÃ§Ã£o de sequÃªncia (para poder classificar as sentenÃ§as como positivas ou negativas).

    model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

    outputs = model(**inputs)

    print(outputs.logits.shape)

SaÃ­da relevante da fase 2 ğŸ“:

    torch.Size([2, 2])

â„¹ï¸Nota:  a cabeÃ§a do modelo toma como entrada os vetores de alta dimensÃ£o que vimos antes e produz vetores contendo dois valores (um por rÃ³tulo)

â„¹ï¸Nota: Como temos apenas duas frases e dois rÃ³tulos, o resultado que obtemos do nosso modelo tem o formato 2 x 2.


## Fase 3: PÃ³s-processamento da saÃ­da

    import torch

    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
    print(predictions)

    print(outputs.logits) ##LÃ³gits

    print(model.config.id2label) ##Probabilidades

    model.config.id2label ##Resultado

SaÃ­da relevante da fase 3ğŸ“:

    ##logits

    tensor([[-1.5607,  1.6123],
            [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)

    ##Probabilidades

    tensor([[4.0195e-02, 9.5980e-01],
            [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)

    ##Resultado
            {0: 'NEGATIVE', 1: 'POSITIVE'}

â„¹ï¸Nota: A saÃ­da sÃ£o logits, pontuaÃ§Ãµes brutas e nÃ£o normalizadas emitidas pela Ãºltima camada do modelo, precisam passar por uma camda SoftMax para serem convertidos em probabilidades


âœ¨âœ¨ExercÃ­cio hugging face: âœï¸ Experimente! Escolha dois (ou mais) textos prÃ³prios e execute-os no sentiment-analysis pipeline. Em seguida, replique vocÃª mesmo os passos que viu aqui e verifique se obtÃ©m os mesmos resultados!

    ## Resposta 

    ## Fase 0

    from transformers import pipeline

    classifier = pipeline("sentiment-analysis")
    result = classifier(
        [
            "The food smels bad!, give me another dish",
            "So, the fish that i order is so good, can i repeat de order ?",
            "I love my work, this years are the greatest os my life",
            "Man you play for four hours, your brain are a beautiful",
            "I feel so sick, please buy me a remedy",
            
        ]
    )

    print(result)


    ## Fase 1

    from transformers import AutoTokenizer


    checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)


    raw_inputs = [
            "So, the fish that i order is so good, can i repeat de order ?",
            "I love my work, this years are the greatest os my life",
            "Man you play for four hours, your brain are a beautiful",
            "I feel so sick, please buy me a remedy",
    ]
    inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
    print(inputs)

    ## Fase 2

    from transformers import AutoModel

    ##checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
    ##checkpoint ja foi carregado mais acima 

    model = AutoModel.from_pretrained(checkpoint)

    outputs = model(**inputs)
    print(outputs.last_hidden_state.shape)

    ## A saÃ­da sera trÃªs nÃºmeros que tem o significado abaixo:
    ## Tamanho do lote: O nÃºmero de sequÃªncias processadas por vez (2 em nosso exemplo).
    ## Comprimento da sequÃªncia: O comprimento da representaÃ§Ã£o numÃ©rica da sequÃªncia (16 no nosso exemplo).
    ## Tamanho oculto: A dimensÃ£o vetorial de cada entrada do modelo.

    ## Fase 2.1

    from transformers import AutoModelForSequenceClassification

    ## Modelo com um cabeÃ§alho de classificaÃ§Ã£o de sequÃªncia (para poder classificar as sentenÃ§as como positivas ou negativas).
    model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
    outputs = model(**inputs)
    print(outputs.logits.shape)
    ## A cabeÃ§a do modelo toma como entrada os vetores de alta dimensÃ£o que vimos antes e produz vetores contendo dois valores (um por rÃ³tulo)
    ## Como temos apenas duas frases e dois rÃ³tulos, o resultado que obtemos do nosso modelo tem o formato 2 x 2.

    ## Fase 3

    print(outputs.logits)

    ## SerÃ£o logits, as pontuaÃ§Ãµes brutas e nÃ£o normalizadas emitidas pela Ãºltima camada do modelo.
    ##precisam passar por uma camda SoftMax para serem convertidos em probabilidades

    import torch

    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
    print(predictions)

    print(model.config.id2label)

## Modulo 3:  Modelos ğŸ“„ğŸ“š

## Criando um transformer ğŸ§ .

Utilizamos

    from transformers import AutoModel

    model = AutoModel.from_pretrained("bert-base-cased")

â„¹ï¸Nota: O mÃ©todo baixarÃ¡ e armazenarÃ¡ em cache os dados do modelo do Hugging Face Hub.

â„¹ï¸Nota: O nome do ponto de verificaÃ§Ã£o corresponde a uma arquitetura e pesos de modelo especÃ­ficos, neste caso um modelo BERT com uma arquitetura bÃ¡sica (12 camadas, 768 tamanhos ocultos, 12 cabeÃ§as de atenÃ§Ã£o)

â„¹ï¸Nota: O modelo possui entradas de caixa (DistinÃ§Ã£o entra maiÃºsculas e minÃºsculas)

â„¹ï¸Nota: O AutoModel class e seus associados sÃ£o, na verdade, wrappers (empacotadores) simples projetados para buscar a arquitetura de modelo apropriada para um determinado ponto de verificaÃ§Ã£o.

â„¹ï¸Nota: Ã‰ uma classe automÃ¡tica que adivinha a arquitetura de modelo apropriada para vocÃª e instanciarÃ¡ a classe de modelo correta.

**PorÃ©m caso saibamos a qual modelo usar podemos optar por:**

    from transformers import BertModel

    model = BertModel.from_pretrained("bert-base-cased")

## Carregando e salvando

Adicionamos no cÃ³digo:

    model.save_pretrained("nome_do_local_de_salvamento")

â„¹ï¸Nota: No diretorio foi criado dois documentos de configuraÃ§Ãµes: config.json e pytorch_model.bin (model safetensors)

â„¹ï¸Nota: No config.json temos os atributos necessÃ¡rios para construir a arquitetura do modelo, metadados, versÃ£o do transformer do ponto de verificaÃ§Ã£o que foi salvo.

â„¹ï¸Nota: No pytorch_model.bin (model safetensors) temos o dicionÃ¡rio de estados, aqui temos todos os pesos do modelo (parÃ¢metros do modelo)

â„¹ï¸Nota: Os dois arquivos ficam juntos. 

â„¹ï¸Nota: Para reutilizar um modelos salvo podemos usar:

    from transformers import AutoModel

    model = AutoModel.from_pretrained("directory_on_my_computer")

## Compartilhando modelos no hugging face

Utilizamos (Um por vez):

    huggingface-cli login

    model.push_to_hub("my-awesome-model")

â„¹ï¸ExplicaÃ§Ã£o: Logo isso farÃ¡ upload dos arquivos do modelo para o Hub, em um repositÃ³rio sob seu namespace chamado my-awesome-model. EntÃ£o, qualquer um pode carregar seu modelo com o from_pretrained() mÃ©todo!

**Para importar os modelos atualizados utilizamos**

    from transformers import AutoModel

    model = AutoModel.from_pretrained("your-username/my-awesome-model")

## Codificado o texto

    from transformers import AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

    encoded_input = tokenizer("Hello, I'm a single sentence!")
    print(encoded_input)


    texto_decodificado = tokenizer.decode(encoded_input["input_ids"][4])

    print(texto_decodificado)

SaÃ­da relevanteğŸ“:

    {'input_ids': [101, 8667, 117, 146, 112, 182, 170, 1423, 5650, 106, 102],
    
    'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
    
    'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
    
    [CLS] Hello, I ' m a single sentence! [SEP]




â„¹ï¸Nota: A saÃ­da desse cÃ³digo serÃ¡ Um dicionÃ¡rio com os seguintes campos:

input_ids: representaÃ§Ãµes numÃ©ricas dos seus tokens

token_type_ids: eles informam ao modelo qual parte da entrada Ã© a frase A e qual Ã© a frase 

attention_mask: indica quais tokens devem ser atendidos e quais nÃ£o devem

â„¹ï¸Nota: Podemos decodificar os IDs de entrada para recuperar o texto original usando:

    texto_decodificado = tokenizer.decode(encoded_input["input_ids"][4])

    print(texto_decodificado)

â„¹ï¸Nota:AtenÃ§Ã£o, entre colchetes no decodificador estÃ¡ a posiÃ§Ã£o que serÃ¡ decodificada pode ser deixado vazio e serÃ¡ decodificado tudo


â„¹ï¸Nota: [CLS] e [SEP] sÃ£o tokens especiais exigidos pelo modelo, sendo que nem todos precisam.


**Com varias frases**

    from transformers import AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

    encoded_input = tokenizer(["How are you?", "I'm fine, thank you"])
    print(encoded_input)

SaÃ­da relevanteğŸ“:

    {'input_ids': 
    
    [[101, 1731, 1132, 1128, 136, 102], 
    
    [101, 146, 112, 182, 2503, 117, 6243, 1128, 102]],
    
    'token_type_ids': 
     
    [[0, 0, 0, 0, 0, 0], 
     
    [0, 0, 0, 0, 0, 0, 0, 0, 0]],
     
    'attention_mask': 
    
    [[1, 1, 1, 1, 1, 1], 
    
    [1, 1, 1, 1, 1, 1, 1, 1, 1]]}

**Extra: podemos pedir ao tokenizador para retornar tensores diretamente do PyTorch **

    encoded_input = tokenizer("How are you?", "I'm fine, thank you!", return_tensors="pt")
    print(encoded_input)

SaÃ­da relevanteğŸ“:

    {'input_ids': 
    
    tensor
    
    ([[  101,  1731,  1132,  1128,   136,   102],

    [  101,  1045,  1005,  1049,  2503,   117,  5763,  1128,   136,   102]]),

    'token_type_ids': 
    
    tensor
    
    ([[0, 0, 0, 0, 0, 0],
    
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 

    'attention_mask': 
    
    tensor
    
    ([[1, 1, 1, 1, 1, 1],


    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}

â„¹ï¸Nota: as duas listas nÃ£o tÃªm o mesmo comprimento! Matrizes e tensores precisam ser retangulares, entÃ£o nÃ£o podemos simplesmente converter essas listas em um tensor PyTorch (ou matriz NumPy). O tokenizador oferece uma opÃ§Ã£o para isso: preenchimento.

**Logo**:

    encoded_input = tokenizer(
        ["How are you?", "I'm fine, thank you!"], padding=True, return_tensors="pt"
    )
    print(encoded_input)

    SaÃ­da relevanteğŸ“:

    {'input_ids': 
    
    tensor
    
    ([[101,  1731,  1132,  1128,   136,   102,     0,     0,     0,     0],
         
         
    [101,  1045,  1005,  1049,  2503,   117,  5763,  1128,   136,   102]]), 

    'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],

    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 

    'attention_mask': 
    
    tensor
    
    ([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],


    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}


## Entradas de preenchimento (padding)

    from transformers import AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

    encoded_input = tokenizer(["How are you?", "I'm fine, thank you"], padding = True, return_tensors = "pt")
    print(encoded_input)

SaÃ­da relevanteğŸ“:

    {'input_ids': 
    
    tensor
    
    ([[ 101, 1731, 1132, 1128,  136,  102,    0,    0,    0],

    [ 101,  146,  112,  182, 2503,  117, 6243, 1128,  102]]),
    
    'token_type_ids': 
    
    tensor
    
    ([[0, 0, 0, 0, 0, 0, 0, 0, 0],

    [0, 0, 0, 0, 0, 0, 0, 0, 0]]),

    'attention_mask': tensor
    
    ([[1, 1, 1, 1, 1, 1, 0, 0, 0],

    [1, 1, 1, 1, 1, 1, 1, 1, 1]])}

**Logo: Agora temos tensores retangulares! os tokens de preenchimento foram codificados em IDs de entrada com ID 0 e tambÃ©m tÃªm um valor de mÃ¡scara de atenÃ§Ã£o de 0. Isso ocorre porque esses tokens de preenchimento nÃ£o devem ser analisados pelo modelo: eles nÃ£o fazem parte da frase real.**

## Truncando entradas 

**BERT sÃ³ foi prÃ©-treinado com sequÃªncias de atÃ© 512 token**

    from transformers import AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

    encoded_input = tokenizer(
        "This is a very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very long sentence.",
        truncation=True,
    )
    print(encoded_input["input_ids"])


SaÃ­da relevanteğŸ“:

    [101, 1188, 1110, 170, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1263, 5650, 119, 102]


## Combinar os argumentos de preenchimento e truncamento, vocÃª pode garantir que seus tensores tenham o tamanho exato necessÃ¡rio:

    from transformers import AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

    encoded_input = tokenizer(
        ["How are you?", "I'm fine, thank you!"],
        padding=True,
        truncation=True,
        max_length=5,
        return_tensors="pt",
    )
    print(encoded_input)

SaÃ­da relevanteğŸ“:

    {'input_ids': tensor
    
    ([[ 101, 1731, 1132, 1128,  102],

    [ 101,  146,  112,  182,  102]]), 

    'token_type_ids': tensor
    
    ([[0, 0, 0, 0, 0],

    [0, 0, 0, 0, 0]]), 
    
    'attention_mask': 
    
    tensor
    
    ([[1, 1, 1, 1, 1],

    [1, 1, 1, 1, 1]])}

## Sobre os tokens especiais :
**SÃ£o usados quando o modelo Ã© treinado com eles**

    from transformers import AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

    encoded_input = tokenizer("How are you?")
    print(encoded_input["input_ids"])
    decoded_input = tokenizer.decode(encoded_input["input_ids"])
    print(decoded_input)

SaÃ­da relevanteğŸ“:

    [101, 1731, 1132, 1128, 136, 102]
    [CLS] How are you? [SEP]

## Extras

    from transformers import BertConfig, BertModel

    # Building the config
    config = BertConfig()

    # Building the model from the config
    model = BertModel(config)   

    print(config)

SaÃ­da relevanteğŸ“:

    'BertConfig {

    "attention_probs_dropout_prob": 0.1,

    "classifier_dropout": null,

    "hidden_act": "gelu",

    "hidden_dropout_prob": 0.1,

    "hidden_size": 768,

    "initializer_range": 0.02,

    "intermediate_size": 3072,

    "layer_norm_eps": 1e-12,

    "max_position_embeddings": 512,

    "model_type": "bert",

    "num_attention_heads": 12,

    "num_hidden_layers": 12,

    "pad_token_id": 0,

    "position_embedding_type": "absolute",

    "transformers_version": "4.55.2",

    "type_vocab_size": 2,

    "use_cache": true,
    
    "vocab_size": 30522
    }'


## Fazendo uso de tensores como entrada para o modelo

    from transformers import BertModel, BertTokenizer
    import torch

    # Carregar o Tokenizador e o Modelo

    tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
    model = BertModel.from_pretrained("bert-base-cased")

    # Opcional: Salvando o modelo para uso futuro

    model.save_pretrained("local_no_computador")

    # As entradas em formato de texto (strings)

    raw_inputs = ["Hello!", "Cool.", "Nice!"]

    # Tokenizar as entradas UMA VEZ
    # A variÃ¡vel 'model_inputs' agora vai conter o dicionÃ¡rio com os tensores de IDs

    model_inputs = tokenizer(raw_inputs, padding=True, return_tensors="pt")

    # Passar os inputs para o modelo
    # Os '**' (unpacking) passam o dicionÃ¡rio 'model_inputs' como argumentos nomeados
    # para o modelo, que espera 'input_ids', 'attention_mask', etc.

    output = model(**model_inputs)

    # O output do modelo agora Ã© vÃ¡lido

    print(output.last_hidden_state.shape)

â„¹ï¸Nota: **Embora o modelo aceite muitos argumentos diferentes, apenas os IDs de entrada sÃ£o necessÃ¡rios.***

SaÃ­da relevanteğŸ“:

    torch.Size([3, 4, 768])

## Modulo 4: tokenizadores, pontos mais importantes.

â„¹ï¸Nota: serÃ¡ explorado o que acontece no pipeline de tokenizaÃ§Ã£o.

PropÃ³sito: traduzir texto em dados (nÃºmeros) que podem ser processados pelo modelo.

â„¹ï¸Nota: Traduzir texto para nÃºmeros Ã© conhecido como codificaÃ§Ã£o., Ã© feita em um processo de duas etapas: a tokenizaÃ§Ã£o, seguida pela conversÃ£o em IDs de entrada.

O primeiro passo Ã© dividir o texto em palavras (tokens), O segundo passo Ã© converter esses tokens em nÃºmeros...

...Para fazer isso, o tokenizador possui um vocabulÃ¡rio, que Ã© a parte que baixamos quando a instanciamos com o from_pretrained() mÃ©todo

**Tokenizador baseado em palavras**

Exemplos:

let's do tokenization!

let/'s/do/tokenization/!

    ## Tokenizadores

    from transformers import BertModel, BertTokenizer

    tokenized_text = "Jim henson was a puppeter".split()
    print(tokenized_text)

SaÃ­da relevanteğŸ“:

    ['Jim', 'henson', 'was', 'a', 'puppeter']

â„¹ï¸Nota: Com esse tipo de tokenizador, podemos acabar com alguns â€œvocabulÃ¡riosâ€ bem grandes, onde um vocabulÃ¡rio Ã© definido pelo nÃºmero total de tokens independentes que temos em nosso corpus.


â„¹ï¸Nota: Cada palavra recebe um ID, comeÃ§ando em 0 e indo atÃ© o tamanho do vocabulÃ¡rio. O modelo usa esses IDs para identificar cada palavra.

â„¹ï¸Nota: Palavras como â€œcÃ£oâ€ sÃ£o representadas de forma diferente de palavras como â€œcÃ£esâ€, e o modelo inicialmente nÃ£o terÃ¡ como saber que â€œcÃ£oâ€ e â€œcÃ£esâ€ sÃ£o semelhantes: ele identificarÃ¡ as duas palavras como nÃ£o relacionadas. O mesmo se aplica a outras palavras semelhantes, como â€œrunâ€ e â€œrunningâ€, que o modelo nÃ£o verÃ¡ como semelhantes inicialmente.

Precisamos de um token personalizado para representar palavras que nÃ£o estÃ£o em nosso vocabulÃ¡rio. Isso Ã© conhecido como token â€œdesconhecidoâ€, geralmente representado como â€[UNK]â€ ou â€<unk>â€.

Caso seja visto muito unk Ã© um mal sinal.

**Tokenizador baseado em caracteres**

â„¹ï¸Nota: O vocabulÃ¡rio Ã© muito menor.
HÃ¡ muito menos tokens fora do vocabulÃ¡rio (desconhecidos), jÃ¡ que cada palavra pode ser construÃ­da a partir de caracteres.

Exemplos:

let's do tokenization!

l/e/t/'/s/d/o/t/o/k/e/n/i/z/a/t/i/o/n/!

â„¹ï¸Nota: Acabaremos com uma quantidade muito grande de tokens a serem processados pelo nosso modelo

Logo usamos uma  tÃ©cnica que combina as duas abordagens: tokenizaÃ§Ã£o de subpalavras.

**Tokenizador baseado em subpalavras**

â„¹ï¸Nota: Aaseiam-se no princÃ­pio de que palavras usadas com frequÃªncia nÃ£o devem ser divididas em subpalavras menores, mas palavras raras devem ser decompostas em subpalavras significativas.

Exemplos:

let's do tokenization!

let's/do/token/ization/!

**Carregando e salvando tokenizadores**

    from transformers import AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

    tokenizer.save_pretrained("local_no_computador")

    encoded_input = tokenizer(
        "Using a Transformer network is simple"
    )
    print(encoded_input)

SaÃ­da relevanteğŸ“:

    {'input_ids': 
    
    [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102],
    
    'token_type_ids': 
    
    [0, 0, 0, 0, 0, 0, 0, 0, 0],
    
    'attention_mask': 
    
    [1, 1, 1, 1, 1, 1, 1, 1, 1]}

**CodificaÃ§Ã£o**

**TokenizaÃ§Ã£o baseada em subpalavras**

    from transformers import AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

    sequence = "Using a Transformer network is simple"
    tokens = tokenizer.tokenize(sequence)

    print(tokens)

    ids = tokenizer.convert_tokens_to_ids(tokens)

    print(ids)

â„¹ï¸Nota: A conversÃ£o para IDs de entrada Ã© feita pelo convert_tokens_to_ids() mÃ©todo tokenizador, Essas saÃ­das, uma vez convertidas no tensor de estrutura apropriado, podem entÃ£o ser usadas como entradas para um modelo.

SaÃ­da relevanteğŸ“:

    ['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']

    [7993, 170, 13809, 23763, 2443, 1110, 3014]'

â„¹ï¸Nota: Usaremos esses id's para decodificaÃ§Ã£o. 

Hugging face exercise:

âœï¸ Try it out! Replicate the two last steps (tokenization and conversion to input IDs) on the input sentences we used in section 2 (â€œIâ€™ve been waiting for a HuggingFace course my whole life.â€ and â€œI hate this so much!â€). Check that you get the same input IDs we got earlier!

âœï¸ Experimente! Replique as duas Ãºltimas etapas (tokenizaÃ§Ã£o e conversÃ£o para IDs de entrada) nas frases de entrada que usamos na seÃ§Ã£o 2 (â€œEu estive esperando por um curso HuggingFace toda a minha vidaâ€ e â€œEu odeio tanto isso!â€). Verifique se vocÃª obtÃ©m os mesmos IDs de entrada que obtivemos anteriormente!

Resposta abaixo:


    from transformers import AutoTokenizer

    checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)

    raw_inputs = [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
    inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
    print(inputs)


SaÃ­da relevanteğŸ“:

    {'input_ids': 
    
    tensor
    
    ([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,2607,  2026,  2878,  2166,  1012,   102],

    [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,      0,     0,     0,     0,     0,     0]]), 
    
    'attention_mask': 
    
    tensor
    
    ([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],

    [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}

**DecodificaÃ§Ã£o**

    from transformers import AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

    sequence = "Using a Transformer network is simple"
    tokens = tokenizer.tokenize(sequence)

    decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
    print(decoded_string)

SaÃ­da relevanteğŸ“:

    ['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']

    Using a transformer network is simple
    
â„¹ï¸Nota:  mÃ©todo nÃ£o apenas converte os Ã­ndices de volta em tokens, mas tambÃ©m agrupa os tokens que faziam parte das mesmas palavras para produzir uma frase legÃ­vel.

## Modulo 5: manipulando mÃºltiplas sequÃªncias.

    import torch
    from transformers import AutoTokenizer, AutoModelForSequenceClassification

    checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

    sequence = "I've been waiting for a HuggingFace course my whole life."

    tokens = tokenizer.tokenize(sequence)
    ids = tokenizer.convert_tokens_to_ids(tokens)
    input_ids = torch.tensor(ids)

    # Essa linha vai falhar
    model(input_ids)

SaÃ­da relevanteğŸ“:

    IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)

â„¹ï¸Nota:O problema Ã© que enviamos uma Ãºnica sequÃªncia para o modelo, enquanto os modelos ğŸ¤— Transformers esperam mÃºltiplas sentenÃ§as por padrÃ£o. O tokenizador nÃ£o apenas converteu a lista de IDs de entrada em um tensor, mas tambÃ©m adicionou uma dimensÃ£o a ele:

**CorreÃ§Ã£o**

    import torch
    from transformers import AutoTokenizer, AutoModelForSequenceClassification

    checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

    sequence = "I've been waiting for a HuggingFace course my whole life."

    tokens = tokenizer.tokenize(sequence)
    ids = tokenizer.convert_tokens_to_ids(tokens)
    input_ids = torch.tensor(ids)

    # CorreÃ§Ã£o
    tokenized_inputs = tokenizer(sequence, return_tensors="pt")
    print(tokenized_inputs["input_ids"])

SaÃ­da relevanteğŸ“:

    tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102]])

    
**Adicionando novas dimensÃµes**

    import torch
    from transformers import AutoTokenizer, AutoModelForSequenceClassification

    checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

    sequence = "I've been waiting for a HuggingFace course my whole life."

    tokens = tokenizer.tokenize(sequence)
    ids = tokenizer.convert_tokens_to_ids(tokens)

    input_ids = torch.tensor([ids])
    print("Input IDs:", input_ids)

    output = model(input_ids)
    print("Logits:", output.logits)

SaÃ­da relevanteğŸ“:

    Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,
    2026,  2878,  2166,  1012]])
    Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)

**Batching(loteamento):  o ato de enviar vÃ¡rias frases atravÃ©s do modelo, todas de uma sÃ³ vez. Se vocÃª tiver apenas uma frase, poderÃ¡ simplesmente criar um lote com uma Ãºnica sequÃªncia:**

    batched_ids = [ids, ids]

Este Ã© um lote de duas sequÃªncias idÃªnticas!

ExercÃ­cio hugging face

âœï¸ Experimente! Converta isso batched_ids liste em um tensor e passe-o pelo seu modelo. Verifique se vocÃª obtÃ©m os mesmos logits de antes (mas duas vezes)!

**Preenchendo as entradas**

    batched_ids = [
        [200, 200, 200],
        [200, 200]
    ]

â„¹ï¸Nota: A lista nÃ£o esta retangular para contornar isso, usaremos enchimento (padding_id) para fazer com que nossos tensores tenham formato retangular. 

â„¹ï¸Nota: Por exemplo, se vocÃª tiver 10 frases com 10 palavras e 1 frase com 20 palavras, o preenchimento garantirÃ¡ que todas as frases tenham 20 palavras. No nosso exemplo, o tensor resultante Ã© assim:

    padding_id = 100

    batched_ids = [
        [200, 200, 200],
        [200, 200, padding_id],
    ]

â„¹ï¸Nota: O ID do token de preenchimento pode ser encontrado em tokenizer.pad_token_id. Vamos usÃ¡-lo e enviar nossas duas frases atravÃ©s do modelo individualmente e agrupadas:

    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    import torch

    checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

    sequence1_ids = [[200, 200, 200]]
    sequence2_ids = [[200, 200]]
    batched_ids = [
        [200, 200, 200],
        [200, 200, tokenizer.pad_token_id],
    ]

    print(model(torch.tensor(sequence1_ids)).logits)
    print(model(torch.tensor(sequence2_ids)).logits)
    print(model(torch.tensor(batched_ids)).logits)

SaÃ­da relevanteğŸ“:

    tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)
    tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)

    We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.

    tensor([[ 1.5694, -1.3895],
            [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)



â„¹ï¸Nota: HÃ¡ algo errado com os logits em nossas previsÃµes em lote: a segunda linha deve ser igual aos logits da segunda frase, mas temos valores completamente diferentes!

Isso ocorre porque a principal caracterÃ­stica dos modelos Transformer sÃ£o as camadas de atenÃ§Ã£o que contextualizar cada token. Eles levarÃ£o em consideraÃ§Ã£o os tokens de preenchimento, pois atendem a todos os tokens de uma sequÃªncia. Para obter o mesmo resultado ao passar frases individuais de diferentes comprimentos pelo modelo ou ao passar um lote com as mesmas frases e preenchimento aplicados, precisamos dizer a essas camadas de atenÃ§Ã£o para ignorarem os tokens de preenchimento. Isso Ã© feito usando uma mÃ¡scara de atenÃ§Ã£o.

**MÃ¡scaras de atenÃ§Ã£o**

SÃ£o tensores,  1s indicam que os tokens correspondentes devem ser atendidos, e 0s indicam que os tokens correspondentes nÃ£o devem ser atendidos

    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    import torch

    checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

    sequence1_ids = [[200, 200, 200]]
    sequence2_ids = [[200, 200]]
    batched_ids = [
        [200, 200, 200],
        [200, 200, tokenizer.pad_token_id],
    ]

    attention_mask = [
        [1, 1, 1],
        [1, 1, 0],
    ]


    outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
    print(outputs.logits)

SaÃ­da relevanteğŸ“:

    tensor([[ 1.5694, -1.3895],
            [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)

ExercÃ­cio hugging face: âœï¸ Experimente! Aplique a tokenizaÃ§Ã£o manualmente nas duas frases usadas na seÃ§Ã£o 2 (â€œEstive esperando por um curso HuggingFace toda a minha vida.â€ e â€œodeio tanto isso!â€). Passe-os pelo modelo e verifique se vocÃª obtÃ©m os mesmos logits da seÃ§Ã£o 2. Agora agrupe-os usando o token de preenchimento e crie a mÃ¡scara de atenÃ§Ã£o adequada. Verifique se vocÃª obtÃ©m os mesmos resultados ao passar pelo modelo!

**SequÃªncias mais longas**

A maioria dos modelos manipula sequÃªncias de atÃ© 512 ou 1024 tokens e trava quando solicitados a processar sequÃªncias mais longas

Para solucionar:

Use um modelo com um comprimento de sequÃªncia suportado maior.

Trunque suas sequÃªncias.
    






















